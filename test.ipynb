{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "# 2023/10/4    11:00 \n",
    "\n",
    "#######################################################\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "import random \n",
    "import numpy as np \n",
    "import torch.nn.functional as F\n",
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "# device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class MLPDiffusion(nn.Module):    \n",
    "    def __init__(self, d, n_steps):\n",
    "        super(MLPDiffusion,self).__init__()\n",
    "        num_units = d\n",
    "\n",
    "        self.layer1 = nn.Linear(d, num_units, bias=True)\n",
    "        self.layer2 = nn.Linear(num_units, num_units, bias=True)\n",
    "        # self.layer3 = nn.Linear(num_units, num_units)\n",
    "        self.layer4 = nn.Linear(num_units, d, bias=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()                 # self.tanh = nn.Tanh()  relu更容易收敛\n",
    "        # self.bn_layers = nn.ModuleList([nn.BatchNorm1d(num_units) for _ in range(3)])\n",
    "        self.bn_layers = nn.ModuleList([nn.BatchNorm1d(num_units) for _ in range(2)])\n",
    "\n",
    "        # self.step_embeddings = nn.ModuleList([nn.Embedding(n_steps,num_units) for _ in range(3)])\n",
    "        self.step_embeddings = nn.ModuleList([nn.Embedding(n_steps,num_units) for _ in range(2)])\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                if m == self.layer4:\n",
    "                    # Xavier initialization for the layer with sigmoid activation\n",
    "                    init.xavier_uniform_(m.weight)\n",
    "                    if m.bias is not None:\n",
    "                        init.constant_(m.bias, 0)\n",
    "                else:\n",
    "                    # He initialization for layers with ReLU activation\n",
    "                    init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "                    if m.bias is not None:\n",
    "                        init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                init.constant_(m.weight, 1)\n",
    "                init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        for idx, (embedding_layer, bn_layer) in enumerate(zip(self.step_embeddings, self.bn_layers)):\n",
    "            t_embedding = embedding_layer(t)\n",
    "            # x = self.layer1(x) if idx == 0 else self.layer2(x) if idx == 1 else self.layer3(x)\n",
    "            x = self.layer1(x) if idx == 0 else self.layer2(x)\n",
    "            x += t_embedding\n",
    "            x = bn_layer(x)\n",
    "            x = self.relu(x)\n",
    "        \n",
    "        x = self.layer4(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MLPDiffusionWithLambda(nn.Module):\n",
    "    def __init__(self, d, n_steps):\n",
    "        super(MLPDiffusionWithLambda, self).__init__()\n",
    "        self.diffNet = MLPDiffusion(d, n_steps)\n",
    "        # 初始化lambda_weight 为一个较小的正值，例如0.5, 并使其为可学习的参数\n",
    "        self.lambda_weight = nn.Parameter(torch.tensor(0.5))  # 如果是6的话得写成小数形式：6.  \n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        return self.diffNet(x, t)\n",
    "\n",
    "\n",
    "class Diffusion(object):  # 注意：这里的batchsize和GAN里面的顺序不一样\n",
    "    def __init__(self, dim, lr, epoches, batchsize=32):\n",
    "        # 1. 实例化的参数\n",
    "        self.dim = dim \n",
    "        self.batchsize = batchsize \n",
    "        self.lr = lr \n",
    "        self.epoches = epoches \n",
    "\n",
    "\n",
    "        # 2. 设置一些参数\n",
    "        self.num_steps = 100    # 即T,对于步骤，一开始可以由beta, 分布的均值和标准差来共同确定\n",
    "        self.betas = torch.linspace(-6, 6, self.num_steps)#制定每一步的beta, size:100\n",
    "        self.betas = torch.sigmoid(self.betas)*(0.5e-2 - 1e-5)+1e-5   \n",
    "        # beta是递增的，最小值为0.00001,最大值为0.005, sigmooid func\n",
    "        # 像学习率一样的一个东西，而且是一个比较小的值，所以就有理由假设逆扩散过程也是一个高斯分布\n",
    "        #计算alpha、alpha_prod、alpha_prod_previous、alpha_bar_sqrt等变量的值\n",
    "        self.alphas = 1 - self.betas    # size: 100\n",
    "        self.alphas_prod = torch.cumprod(self.alphas, 0)    # size: 100\n",
    "        # 就是让每一个都错一下位\n",
    "        self.alphas_prod_p = torch.cat([torch.tensor([1]).float(), self.alphas_prod[:-1]],0)  # p表示previous  \n",
    "        # alphas_prod[:-1] 表示取出 从0开始到倒数第二个值\n",
    "        self.alphas_bar_sqrt = torch.sqrt(self.alphas_prod)\n",
    "        self.one_minus_alphas_bar_log = torch.log(1 - self.alphas_prod)\n",
    "        self.one_minus_alphas_bar_sqrt = torch.sqrt(1 - self.alphas_prod)\n",
    "\n",
    "        assert self.alphas.shape == self.alphas_prod.shape == self.alphas_prod_p.shape ==\\\n",
    "        self.alphas_bar_sqrt.shape == self.one_minus_alphas_bar_log.shape\\\n",
    "        == self.one_minus_alphas_bar_sqrt.shape\n",
    "\n",
    "\n",
    "        # 3.初始化去噪模型\n",
    "        self.Denoise = MLPDiffusionWithLambda(self.dim, self.num_steps)\n",
    "\n",
    "        # 4.损失函数\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "        # 5.优化器\n",
    "        # weight_decay=1e-5   添加L2正则化，权重衰减     self.lr = 0.005\n",
    "        self.optimizer = optim.Adam(self.Denoise.parameters(), lr=self.lr, weight_decay=1e-5)\n",
    "\n",
    "    #前向加噪过程，计算任意时刻加噪后的xt，基于x_0和重参数化\n",
    "    def q_x(self, x_0, t, center, cov):\n",
    "        \"\"\"可以基于x[0]得到任意时刻t的x[t]\"\"\"\n",
    "        \n",
    "        noise = np.random.multivariate_normal(center, cov, x_0.shape[0])  \n",
    "        noise = torch.from_numpy(np.maximum(np.minimum(noise, np.ones(( x_0.shape[0], self.dim))),\n",
    "                                             np.zeros(( x_0.shape[0], self.dim)))).float()\n",
    "\n",
    "\n",
    "        # noise = torch.randn_like(x_0)   # noise是从某分布中生成的随机噪声\n",
    "        alphas_t = self.alphas_bar_sqrt[t]\n",
    "        alphas_1_m_t = self.one_minus_alphas_bar_sqrt[t]\n",
    "\n",
    "        xt = alphas_t * x_0 + alphas_1_m_t * noise\n",
    "        return xt #在x[0]的基础上添加噪声\n",
    "        # 上面就可以通过x0和t来采样出xt的值\n",
    "\n",
    "    def diffusion_loss_fn(self, x_0, negative_samples, center, cov):\n",
    "        ''' \n",
    "        x_0: positive_samples\n",
    "        '''\n",
    "        # 使用ReLU确保lambda_weight始终为正\n",
    "        lambda_value = torch.relu(self.Denoise.lambda_weight)\n",
    "\n",
    "        # 为了确保正样本的损失（loss_positive）在整体损失中占有更大的权重\n",
    "        # 模型的目标是更加关注正样本，并尽可能让生成的样本远离负样本\n",
    "        # 使用clamp确保lambda_weight在[a, b]范围内  \n",
    "        lambda_weight = torch.clamp(lambda_value, min=0.5, max=0.1)\n",
    "\n",
    "        # n_steps为中的时间步数，这里是500步\n",
    "        batch_size = x_0.shape[0]\n",
    "        n_steps = self.num_steps\n",
    "\n",
    "        x_0 = torch.from_numpy(x_0).float()\n",
    "        negative_samples = torch.from_numpy(negative_samples).float()\n",
    "\n",
    "        t = torch.full((batch_size,), n_steps-1)\n",
    "        t = t.unsqueeze(-1)\n",
    "\n",
    "        xt = self.q_x(x_0, t, center, cov)\n",
    "\n",
    "        output = self.Denoise(xt, t.squeeze(-1))\n",
    "        # epsilon = (xt - x_0 * self.alphas_bar_sqrt[t]) / self.one_minus_alphas_bar_sqrt[t]  \n",
    "        # 根据公式反推epsilon, 以便进行损失计算\n",
    "        # return (epsilon - output).square().mean()\n",
    "\n",
    "        # 正样本的重建误差\n",
    "        loss_positive = (x_0 - output).square().mean()\n",
    "\n",
    "        # 负样本的L2距离\n",
    "        differences = output.unsqueeze(1) - negative_samples\n",
    "        loss_negative = -torch.mean((differences ** 2).sum(dim=2))\n",
    "\n",
    "        # 负样本的余弦距离\n",
    "        # cosine_dists = 1.0 - F.cosine_similarity(output.unsqueeze(1), negative_samples, dim=2)\n",
    "        # loss_negative = torch.mean(cosine_dists)\n",
    "\n",
    "        # loss_negative = -torch.mean(torch.norm(output.unsqueeze(1) - negative_samples, dim=2)**2)\n",
    "\n",
    "        # loss3: 计算两两之间的距离\n",
    "        distances = torch.cdist(output, output)\n",
    "        # 将对角线上的值(每个解与自身的距离)设置为0\n",
    "        mask = torch.eye(len(output)) == 1\n",
    "        distances.masked_fill(mask, 0)\n",
    "        #计算每个解的多样性度量，即与其他解的平均距离\n",
    "        diversity_measures = distances.sum(1) / (len(output) - 1)\n",
    "        # 计算整体的多样性度量\n",
    "        overall_diversity = diversity_measures.mean()\n",
    "        # alpha是一个超参数，表示多样性正则化的权重\n",
    "        alpha = -0.1  # 注意这里的 alpha为负，是为了在total_loss减去多样性度量，因为我们希望鼓励更大的多样性\n",
    "\n",
    "        # 整合损失 = positive + negative  + diversity\n",
    "        total_loss = loss_positive + lambda_weight * loss_negative + alpha * overall_diversity\n",
    "\n",
    "        # return (x_0 - output).square().mean()\n",
    "        return total_loss\n",
    "\n",
    "    def train(self, positive_samples, negative_samples):\n",
    "        # def train(self, pop_dec, samples_pool):\n",
    "        ''' \n",
    "        pop_dec: shape(32, 31)   \n",
    "        samples_pool.shape=(10, 31)是当前种群中表现最好的10个解，计算他们的均值和方差，用以生成随机噪声，即作为随机噪声的均值和方差\n",
    "        \n",
    "        positive_sample 用于训练正样本，并且作为center, cov的标尺, 这里只选前10个样本用于训练\n",
    "\n",
    "        negative_sample 用于训练的负样本，余下的90个样本\n",
    "        '''\n",
    "        self.Denoise.train()\n",
    "        n, d = np.shape(positive_samples)\n",
    "        indices = np.arange(n)  # indices=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9,..., 30, 31])\n",
    "        \n",
    "        center = np.mean(positive_samples[:10, :], axis=0)  # (31,1)  axis=0，对第一个维度求均值    下面的 cov 矩阵提供了一个关于这10个样本在31个特征上相互关系的全面视图。\n",
    "        cov = np.cov(positive_samples[:10, :].reshape((d, positive_samples[:10, :].size // d)))#  (10, 31)->(31, 10)  conv=(31,31)  np.cov 函数用于计算协方差矩阵   samples_pool.shape=(10, 31),   \n",
    "        \n",
    "        # 定义目标样本数量\n",
    "        target_samples = 50\n",
    "\n",
    "        # 过采样正样本至50个\n",
    "        upsampled_positive_samples = resample(positive_samples, \n",
    "                                            replace=True, \n",
    "                                            n_samples=target_samples,\n",
    "                                            random_state=123)\n",
    "\n",
    "        # 欠采样负样本至50个\n",
    "        downsampled_negative_samples = resample(negative_samples, \n",
    "                                        replace=False, \n",
    "                                        n_samples=target_samples,\n",
    "                                        random_state=123)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        for epoch in range(self.epoches):\n",
    "            loss = 0\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.diffusion_loss_fn(upsampled_positive_samples, downsampled_negative_samples, center, cov)\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.Denoise.parameters(), 1.)\n",
    "            self.optimizer.step()\n",
    "            print(\"Epoch[{}], loss: {:.5f}\".format(epoch, loss))\n",
    "\n",
    "        random.shuffle(indices)\n",
    "        positive_samples = positive_samples[indices, :]   # 感觉这里应该加上label = labels[indices, :]\n",
    "\n",
    "\n",
    "    def p_sample_loop(self, x_T):\n",
    "        # \"\"\"从x[T]恢复x[T-1]、x[T-2]|...x[0]\"\"\"\n",
    "        cur_x = x_T\n",
    "\n",
    "        x_0 = self.p_sample(cur_x, self.num_steps - 1)\n",
    "        return x_0\n",
    "\n",
    "    def p_sample(self, x, t): # 参数重整化的过程\n",
    "        \"\"\"从x[t]采样t-1时刻的重构值，即从x[t]采样出x[t-1]\"\"\"\n",
    "        t = torch.tensor([t])\n",
    "        x_0 = self.Denoise(x,t)\n",
    "\n",
    "        return x_0 \n",
    "    \n",
    "\n",
    "    def generate(self, sample_noises, population_size):# population_size=100\n",
    "        self.Denoise.eval()\n",
    "        center = np.mean(sample_noises, axis=0).T\n",
    "        cov = np.cov(sample_noises.T)\n",
    "\n",
    "        noises = np.random.multivariate_normal(center, cov, population_size)\n",
    "        noises = torch.from_numpy(np.maximum(np.minimum(noises, np.ones((population_size, self.dim))),\n",
    "                                             np.zeros((population_size, self.dim)))).float()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # decs= self.p_sample_loop(Variable(noises.cpu()).float(), center, cov).cpu().data.numpy()\n",
    "            decs= self.p_sample_loop(Variable(noises.cpu()).float()).cpu().data.numpy()\n",
    "    \n",
    "        return decs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 假设你有一个批次的生成解，维度为(batch_size, n_features)\n",
    "generated_solutions = torch.randn((batch_size, n_features))\n",
    "\n",
    "# 计算两两之间的距离\n",
    "distances = torch.cdist(generated_solutions, generated_solutions)\n",
    "\n",
    "# 将对角线上的值（每个解与自身的距离）设置为0\n",
    "mask = torch.eye(batch_size) == 1\n",
    "distances.masked_fill_(mask, 0)\n",
    "\n",
    "# 计算每个解的多样性度量，即与其他解的平均距离\n",
    "diversity_measures = distances.sum(1) / (batch_size - 1)\n",
    "\n",
    "# 计算整体的多样性度量\n",
    "overall_diversity = diversity_measures.mean()\n",
    "\n",
    "# 假设你的主要损失为loss_main，然后将多样性正则化纳入总损失中\n",
    "alpha = 0.1  # 这是一个超参数，表示多样性正则化的权重\n",
    "total_loss = loss_main - alpha * overall_diversity  # 注意，这里是减去多样性度量，因为我们希望鼓励更大的多样性\n",
    "\n",
    "# 接下来，你可以使用该损失来进行模型的更新\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "a = torch.randn(2,3)\n",
    "print(len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import numpy as np \n",
    "from sklearn.utils import resample\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# 生成一个形状为(100, 31)的随机数组\n",
    "random_array = np.random.rand(100, 31)\n",
    "\n",
    "# 沿着数组的第二个轴（axis=1）计算每行的和\n",
    "row_sums = np.sum(random_array, axis=1)\n",
    "\n",
    "# 使用NumPy的广播（broadcasting）机制进行规范化\n",
    "normalized_array = random_array / row_sums[:, np.newaxis]\n",
    "\n",
    "# 指定positive_samples的索引\n",
    "positive_indices = [0, 2, 5, 9, 10, 11, 30, 22, 32, 12]\n",
    "\n",
    "# 创建包含所有索引的列表\n",
    "all_indices = list(range(100))\n",
    "\n",
    "# 从all_indices中移除positive_indices \n",
    "negative_indices = list(set(all_indices) - set(positive_indices))\n",
    "\n",
    "# 提取positive_samples 和 negative_samples \n",
    "positive_samples = normalized_array[positive_indices, :]\n",
    "negative_samples = normalized_array[negative_indices, :]\n",
    "\n",
    "\n",
    "\n",
    "# 假设你已经生成了positive_samples\n",
    "# 为了使用SMOTE，我们还需要标签\n",
    "# y_positive = [1] * len(positive_samples)\n",
    "\n",
    "# # 临时添加一个负样本和其对应的标签\n",
    "# X_temp = np.vstack([positive_samples, np.zeros((1, positive_samples.shape[1]))])\n",
    "# y_temp = y_positive + [0]\n",
    "\n",
    "# # 计算过采样比例\n",
    "# oversampling_ratio = (3 * len(positive_samples) - len(positive_samples)) / len(positive_samples)\n",
    "\n",
    "# # 使用SMOTE\n",
    "# smote = SMOTE(sampling_strategy=oversampling_ratio,  \n",
    "#               random_state=42, \n",
    "#               k_neighbors=5)\n",
    "\n",
    "# X_resampled, y_resampled = smote.fit_resample(X_temp, y_temp)\n",
    "\n",
    "# 现在，删除临时的负样本并只选择正样本\n",
    "# X_resampled = X_resampled[y_resampled == 1]\n",
    "# y_resampled = y_resampled[y_resampled == 1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 你还需要对应的标签\n",
    "y_positive = [1] * len(positive_samples)\n",
    "y_negative = [0] * len(negative_samples[:50, :])\n",
    "# negative_samples = normalized_array[negative_indices, :]\n",
    "\n",
    "# 拼接数据\n",
    "X = np.vstack((positive_samples, negative_samples[:50, :]))\n",
    "y = np.array(y_positive + y_negative)\n",
    "\n",
    "# 使用SMOTE\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42, k_neighbors=5)  # 设置k_neighbors为一个合适的值\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# 现在，删除临时的负样本并只选择正样本\n",
    "X_resampled = X_resampled[y_resampled == 1]\n",
    "y_resampled = y_resampled[y_resampled == 1]\n",
    "\n",
    "# X_resampled和y_resampled现在包含了原始数据和合成的正样本\n",
    "# 你可以选择从X_resampled中分离出增强的正样本，如果需要的话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 31)\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(X_resampled.shape)\n",
    "print(y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义目标样本数量\n",
    "target_samples = 50\n",
    "\n",
    "# 过采样正样本至50个\n",
    "upsampled_positive_samples = resample(positive_samples, \n",
    "                                    replace=True, \n",
    "                                    n_samples=target_samples,\n",
    "                                    random_state=123)\n",
    "\n",
    "# 欠采样负样本至50个\n",
    "downsampled_negative_samples = resample(negative_samples, \n",
    "                                replace=False, \n",
    "                                n_samples=target_samples,\n",
    "                                random_state=123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(50, 31)\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "print(type(upsampled_positive_samples))\n",
    "print(upsampled_positive_samples.shape)\n",
    "unique_rows = np.unique(upsampled_positive_samples, axis=0)\n",
    "num_unique_samples = unique_rows.shape[0]\n",
    "print(num_unique_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Diffusion(31, 0.001, 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0], loss: -0.58181\n",
      "Epoch[1], loss: -0.59182\n",
      "Epoch[2], loss: -0.60161\n",
      "Epoch[3], loss: -0.60466\n",
      "Epoch[4], loss: -0.60684\n",
      "Epoch[5], loss: -0.62004\n",
      "Epoch[6], loss: -0.62330\n",
      "Epoch[7], loss: -0.63559\n",
      "Epoch[8], loss: -0.63724\n",
      "Epoch[9], loss: -0.64483\n",
      "Epoch[10], loss: -0.65134\n",
      "Epoch[11], loss: -0.65943\n",
      "Epoch[12], loss: -0.66600\n",
      "Epoch[13], loss: -0.67557\n",
      "Epoch[14], loss: -0.68218\n",
      "Epoch[15], loss: -0.69517\n",
      "Epoch[16], loss: -0.69803\n",
      "Epoch[17], loss: -0.69756\n",
      "Epoch[18], loss: -0.71543\n",
      "Epoch[19], loss: -0.71731\n",
      "Epoch[20], loss: -0.72729\n",
      "Epoch[21], loss: -0.73643\n",
      "Epoch[22], loss: -0.74311\n",
      "Epoch[23], loss: -0.75065\n",
      "Epoch[24], loss: -0.75600\n",
      "Epoch[25], loss: -0.76384\n",
      "Epoch[26], loss: -0.77162\n",
      "Epoch[27], loss: -0.77763\n",
      "Epoch[28], loss: -0.78343\n",
      "Epoch[29], loss: -0.79851\n",
      "Epoch[30], loss: -0.80150\n",
      "Epoch[31], loss: -0.80766\n",
      "Epoch[32], loss: -0.81598\n",
      "Epoch[33], loss: -0.82572\n",
      "Epoch[34], loss: -0.83238\n",
      "Epoch[35], loss: -0.83898\n",
      "Epoch[36], loss: -0.84747\n",
      "Epoch[37], loss: -0.85606\n",
      "Epoch[38], loss: -0.86626\n",
      "Epoch[39], loss: -0.87539\n",
      "Epoch[40], loss: -0.87709\n",
      "Epoch[41], loss: -0.88259\n",
      "Epoch[42], loss: -0.89728\n",
      "Epoch[43], loss: -0.90263\n",
      "Epoch[44], loss: -0.91027\n",
      "Epoch[45], loss: -0.92041\n",
      "Epoch[46], loss: -0.92783\n",
      "Epoch[47], loss: -0.93243\n",
      "Epoch[48], loss: -0.94213\n",
      "Epoch[49], loss: -0.94904\n"
     ]
    }
   ],
   "source": [
    "# net.train(positive_samples, negative_samples)\n",
    "net.train(X_resampled, negative_samples)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moea",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
