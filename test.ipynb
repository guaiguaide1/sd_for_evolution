{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "import random \n",
    "import numpy as np \n",
    "from sklearn.utils import resample\n",
    "import torch.nn.functional as F\n",
    "import os \n",
    "\n",
    "# 矩阵X求梯度，根据梯度来进行数据增强\n",
    "\n",
    "def gradient_M(r):  \n",
    "    \"\"\"计算解x相对于目标M的梯度\"\"\"\n",
    "    return -r.T\n",
    "\n",
    "def gradient_V(X, s, c):\n",
    "    \"\"\"计算解x相对于目标V的梯度\"\"\"\n",
    "    s_expanded = s.T  # 使s的形状为(1, 31)\n",
    "    inter_result = s_expanded * s_expanded * X  # 逐元素乘法\n",
    "    return 2 * np.tensordot(inter_result, c, axes=([1], [0]))  # 矩阵乘法\n",
    "\n",
    "def compute_gradients(X, r, s, c):\n",
    "    grad_M = gradient_M(r)\n",
    "    grad_V = gradient_V(X, s, c)\n",
    "    return grad_M, grad_V\n",
    "def perturb_solution_along_gradient(X, r, s, c, alpha=0.2):\n",
    "    \"\"\"沿着梯度方向进行小幅度扰动\"\"\"\n",
    "    grad_M_val, grad_V_val = compute_gradients(X, r, s, c)\n",
    "    \n",
    "    # 根据两个梯度更新解x。这里的alpha是一个学习率参数，用于控制扰动的大小\n",
    "    new_X = X - alpha * (grad_M_val + 3 * grad_V_val)   # [M, V] = [return, risk]\n",
    "    \n",
    "    # 确保解的每一维数值在[0, 1]之间\n",
    "    new_X = np.clip(new_X, 0, 1)\n",
    "    \n",
    "    # 确保解的31维之和为1\n",
    "    new_X /= new_X.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    return new_X\n",
    "\n",
    "class MLPDiffusion(nn.Module):    \n",
    "    def __init__(self, d, n_steps):\n",
    "        super(MLPDiffusion,self).__init__()\n",
    "        num_units = d\n",
    "\n",
    "        self.layer1 = nn.Linear(d, num_units)\n",
    "        self.layer2 = nn.Linear(num_units, num_units)\n",
    "        # self.layer3 = nn.Linear(num_units, num_units)\n",
    "        self.layer4 = nn.Linear(num_units, d)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()                 # self.tanh = nn.Tanh()  relu更容易收敛\n",
    "        # self.bn_layers = nn.ModuleList([nn.BatchNorm1d(num_units) for _ in range(3)])\n",
    "        self.bn_layers = nn.ModuleList([nn.BatchNorm1d(num_units) for _ in range(2)])\n",
    "\n",
    "        # self.step_embeddings = nn.ModuleList([nn.Embedding(n_steps,num_units) for _ in range(3)])\n",
    "        self.step_embeddings = nn.ModuleList([nn.Embedding(n_steps,num_units) for _ in range(2)])\n",
    "\n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                if m == self.layer4:\n",
    "                    # Xavier initialization for the layer with sigmoid activation\n",
    "                    init.xavier_uniform_(m.weight)\n",
    "                    if m.bias is not None:\n",
    "                        init.constant_(m.bias, 0)\n",
    "                else:\n",
    "                    # He initialization for layers with ReLU activation\n",
    "                    init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "                    if m.bias is not None:\n",
    "                        init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                init.constant_(m.weight, 1)\n",
    "                init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        for idx, (embedding_layer, bn_layer) in enumerate(zip(self.step_embeddings, self.bn_layers)):\n",
    "            t_embedding = embedding_layer(t)\n",
    "            # x = self.layer1(x) if idx == 0 else self.layer2(x) if idx == 1 else self.layer3(x)\n",
    "            x = self.layer1(x) if idx == 0 else self.layer2(x)\n",
    "            x += t_embedding  \n",
    "            # x = bn_layer(x)\n",
    "            x = self.relu(x)\n",
    "        \n",
    "        x = self.layer4(x)\n",
    "        x = F.softmax(x, dim=1)  # 使用softmax确保输出的每个维度的和为1\n",
    "        # x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# 双塔网络结构（Siamese Networks）是指两个完全相同的子网络并行运行，共享相同的权重，\n",
    "# 并对两个输入产生两个输出。这种网络的目的是比较这两个输出，通常用于计算两个输入之间的相似性或差异。\n",
    "# 这种网络结构常用于一系列任务，如人脸验证、签名验证和图像相似性匹配。\n",
    "class SiameseDiffModel(nn.Module):    \n",
    "    def __init__(self, d, n_steps):\n",
    "        super(SiameseDiffModel, self).__init__()\n",
    "        self.diffModel = MLPDiffusion(d, n_steps)\n",
    "\n",
    "        # 初始化可学习的系数 [0.01, 5]     最开始的初始化1, 1, 0.05\n",
    "        self.alpha = nn.Parameter(torch.tensor(20.))  # 如果是6的话得写成小数形式：6.  \n",
    "        self.beta = nn.Parameter(torch.tensor(5.))  # 如果是6的话得写成小数形式：6. \n",
    "        self.gamma = nn.Parameter(torch.tensor(5.)) \n",
    "\n",
    "    def forward_one(self, x, t):\n",
    "        return self.diffModel(x, t)\n",
    "\n",
    "    def forward(self, x1, t1, x2, t2): \n",
    "        # x1和x2可以是正样本和负样本\n",
    "        output1 = self.forward_one(x1, t1)\n",
    "        output2 = self.forward_one(x2, t2)\n",
    "        return output1, output2\n",
    "\n",
    "\n",
    "class Diffusion(object):  # 注意：这里的batchsize和GAN里面的顺序不一样\n",
    "    def __init__(self, dim, lr, epoches, batchsize=8):\n",
    "        # 1. 实例化的参数\n",
    "        self.dim = dim \n",
    "        self.batchsize = batchsize \n",
    "        self.lr = lr \n",
    "        self.epoches = epoches \n",
    "\n",
    "\n",
    "        # 2. 设置一些参数\n",
    "        self.num_steps = 100    # 即T,对于步骤，一开始可以由beta, 分布的均值和标准差来共同确定\n",
    "        self.betas = torch.linspace(-6, 6, self.num_steps)#制定每一步的beta, size:100\n",
    "        self.betas = torch.sigmoid(self.betas)*(0.5e-2 - 1e-5)+1e-5   \n",
    "        # beta是递增的，最小值为0.00001,最大值为0.005, sigmooid func\n",
    "        # 像学习率一样的一个东西，而且是一个比较小的值，所以就有理由假设逆扩散过程也是一个高斯分布\n",
    "        #计算alpha、alpha_prod、alpha_prod_previous、alpha_bar_sqrt等变量的值\n",
    "        self.alphas = 1 - self.betas    # size: 100\n",
    "        self.alphas_prod = torch.cumprod(self.alphas, 0)    # size: 100\n",
    "        # 就是让每一个都错一下位\n",
    "        self.alphas_prod_p = torch.cat([torch.tensor([1]).float(), self.alphas_prod[:-1]],0)  # p表示previous  \n",
    "        # alphas_prod[:-1] 表示取出 从0开始到倒数第二个值\n",
    "        self.alphas_bar_sqrt = torch.sqrt(self.alphas_prod)\n",
    "        self.one_minus_alphas_bar_log = torch.log(1 - self.alphas_prod)\n",
    "        self.one_minus_alphas_bar_sqrt = torch.sqrt(1 - self.alphas_prod)\n",
    "\n",
    "        assert self.alphas.shape == self.alphas_prod.shape == self.alphas_prod_p.shape ==\\\n",
    "        self.alphas_bar_sqrt.shape == self.one_minus_alphas_bar_log.shape\\\n",
    "        == self.one_minus_alphas_bar_sqrt.shape\n",
    "\n",
    "\n",
    "        # 3.初始化去噪模型\n",
    "        # self.Denoise = MLPDiffusion(self.dim, self.num_steps)\n",
    "        self.Denoise = SiameseDiffModel(self.dim, self.num_steps)\n",
    "        # 检查 'best_model.pth' 是否存在\n",
    "        # if os.path.exists('best_model.pth'):\n",
    "        #     # 如果文件存在，加载模型参数\n",
    "        #     checkpoint = torch.load('best_model.pth')\n",
    "        #     self.Denoise.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "        # 4.损失函数\n",
    "        self.MSEloss = nn.MSELoss()\n",
    "\n",
    "        # 5.优化器\n",
    "        # weight_decay=1e-5   添加L2正则化，权重衰减\n",
    "        # self.optimizer = optim.Adam(self.Denoise.parameters(), lr=self.lr, weight_decay=1e-5)\n",
    "        self.optimizer = optim.Adam(self.Denoise.parameters(), lr=self.lr, weight_decay=1e-5)\n",
    "    \n",
    "\n",
    "    #前向加噪过程，计算任意时刻加噪后的xt，基于x_0和重参数化\n",
    "    def q_x(self, x_0, t, center, cov):\n",
    "        \"\"\"可以基于x[0]得到任意时刻t的x[t]\"\"\"\n",
    "\n",
    "        noise = np.random.multivariate_normal(center, cov, x_0.shape[0])  \n",
    "        noise = torch.from_numpy(np.maximum(np.minimum(noise, np.ones(( x_0.shape[0], self.dim))),\n",
    "                                             np.zeros(( x_0.shape[0], self.dim)))).float()\n",
    "\n",
    "        alphas_t = self.alphas_bar_sqrt[t]\n",
    "        alphas_1_m_t = self.one_minus_alphas_bar_sqrt[t]\n",
    "\n",
    "        xt = alphas_t * x_0 + alphas_1_m_t * noise\n",
    "        return xt #在x[0]的基础上添加噪声\n",
    "        # 上面就可以通过x0和t来采样出xt的值\n",
    "\n",
    "    def regularize_loss(self, l1_factor=0.005, l2_factor=0.005):\n",
    "        l1_loss = 0\n",
    "        l2_loss = 0\n",
    "        for param in self.Denoise.parameters():\n",
    "            l1_loss += torch.sum(torch.abs(param))\n",
    "            l2_loss += torch.sum(param ** 2)\n",
    "        \n",
    "        return l1_factor * l1_loss + l2_factor * l2_loss\n",
    "    \n",
    "    def population_entropy(self, population):\n",
    "        \"\"\"\n",
    "        计算种群熵来衡量种群的多样性。\n",
    "        :param population: 一个形状为 [N, D] 的张量，其中 N 是种群大小，D 是解的维度。\n",
    "        :return: 一个标量张量表示种群的熵。\n",
    "        \"\"\"\n",
    "        N, D = population.shape\n",
    "        \n",
    "        # 计算每个个体与种群中其他所有个体的欧氏距离的平方和\n",
    "        distances = torch.norm(population[:, None] - population, dim=2, p=2) ** 2\n",
    "        \n",
    "        # 使用高斯函数进行归一化\n",
    "        p = torch.exp(-distances / (2 * distances.var(dim=1, keepdim=True)))\n",
    "        # p /= p.sum(dim=1, keepdim=True)\n",
    "        p = p / p.sum(dim=1, keepdim=True)\n",
    "\n",
    "        \n",
    "        # 计算整个种群的熵\n",
    "        entropy = -torch.sum(p * torch.log2(p + 1e-10)) / N  # 加上一个小常数防止对零取对数\n",
    "\n",
    "        return entropy\n",
    "\n",
    "    def loss_function(self, x_0_p, x_0_n, output_p, output_n, margin=0.25):# margin=0.5\n",
    "        '''                        \n",
    "        x_0_p: positive_samples                  # 上一个margin为0.8效果不好\n",
    "        x_0_n: negative_samples\n",
    "        output_p:\n",
    "        output_n:\n",
    "        '''\n",
    "\n",
    "        # 使用ReLU确保lambda_weight始终为正\n",
    "        # 为了确保正样本的损失（loss_positive）在整体损失中占有更大的权重\n",
    "        alpha = torch.clamp(torch.relu(self.Denoise.alpha), min=1, max=30)\n",
    "        beta = torch.clamp(torch.relu(self.Denoise.beta), min=0.01, max=20)\n",
    "        gamma = torch.clamp(torch.relu(self.Denoise.gamma), min=0.01, max=20)\n",
    "\n",
    "        # loss1: 正样本的重构损失 \n",
    "        recon_loss =  nn.MSELoss()(x_0_p, output_p) - 1.025 * nn.MSELoss()(x_0_n, output_n)   \n",
    "        \n",
    "        # loss2: 对比损失  \n",
    "        positive_dist = torch.norm(output_p - x_0_p, dim=1)   # 正样本的L1损失\n",
    "        negative_dist = torch.norm(output_p - x_0_n, dim=1)\n",
    "        \n",
    "        # distances = torch.cdist(output_p, output_n)\n",
    "        # average_distances = distances.mean(dim=1)  # 计算每个x_output与所有x_negative之间的平均距离\n",
    "        # overall_average_distance = average_distances.mean()  # 你可能还想计算所有的平均距离的总平均，以用于损失\n",
    "        # negative_dist = overall_average_distance # 这个值可以被用作或者与其他损失结合作为对比损失\n",
    "        contrast_loss = torch.clamp(margin + positive_dist - negative_dist, min=0).mean()\n",
    "        # contrast_loss = torch.clamp(margin + positive_dist - negative_dist, min=0).mean()\n",
    "        # contrast_loss = -negative_dist\n",
    "        \n",
    "        # loss3: 计算正样本两两之间的距离\n",
    "        # distances = torch.cdist(output_p, output_p)\n",
    "        # mask = torch.eye(len(output_p)) == 1# 将对角线上的值(每个解与自身的距离)设置为0\n",
    "        # distances.masked_fill(mask, 0)\n",
    "        # diversity_measures = distances.sum(1) / (len(output_p) - 1)#计算每个解的多样性度量，即与其他解的平均距离\n",
    "        # overall_diversity = -diversity_measures.mean()# 计算整体的多样性度量\n",
    "        # loss3 = overall_diversity  \n",
    "\n",
    "        loss3 =  self.population_entropy(output_p)\n",
    "\n",
    "        # output_p 和 x_0_n的距离也要尽可能地远\n",
    "        # differences = output_p.unsqueeze(1) - x_0_n\n",
    "        # loss_negative = -torch.mean((differences ** 2).sum(dim=2))\n",
    "        \n",
    "        # 正则化（可选）\n",
    "        reg_loss = self.regularize_loss()  # 您可以选择L1、L2或其他形式的正则化\n",
    "        \n",
    "        # total_loss = alpha * recon_loss + beta * contrast_loss + gamma * (loss3) + 0.275 * loss_negative + 0.5 * reg_loss\n",
    "        total_loss = recon_loss + contrast_loss - 8 * loss3 + 0.5 * reg_loss\n",
    "        return total_loss\n",
    " \n",
    "\n",
    "    def diffusion_loss_fn(self, x_0_p, x_0_n, center, cov):\n",
    "        # x_0_p: positive_samples\n",
    "        # x_0_n: negative_samples\n",
    "\n",
    "        # 对正样本处理，n_steps为中的时间步数，这里是100步\n",
    "        batch_size_p = x_0_p.shape[0]\n",
    "        n_steps = self.num_steps\n",
    "        x_0_p = torch.from_numpy(x_0_p).float()\n",
    "        t_p = torch.full((batch_size_p,), n_steps-1)\n",
    "        t_p = t_p.unsqueeze(-1)\n",
    "        xt_p = self.q_x(x_0_p, t_p, center, cov)\n",
    "\n",
    "        # 对负样本处理\n",
    "        \n",
    "        batch_size_n = x_0_n.shape[0]\n",
    "        n_steps = self.num_steps\n",
    "        x_0_n = torch.from_numpy(x_0_n).float()\n",
    "        t_n = torch.full((batch_size_n,), n_steps-1)\n",
    "        t_n = t_n.unsqueeze(-1)\n",
    "        xt_n = self.q_x(x_0_n, t_n, center, cov)\n",
    "\n",
    "\n",
    "        output_p, output_n = self.Denoise(xt_p, t_p.squeeze(-1),  xt_n, t_n.squeeze(-1))  # 这里让模型直接预测x_0而不是噪声\n",
    "\n",
    "        total_loss = self.loss_function(x_0_p, x_0_n, output_p, output_n)\n",
    "    \n",
    "        return total_loss\n",
    "    \n",
    "    def train(self, positive_samples, negative_samples, r, s, c):\n",
    "        ''' \n",
    "        pop_dec: shape(32, 31)    用于训练的数据，这里只选前32个样本用于训练\n",
    "        samples_pool.shape=(10, 31)是当前种群中表现最好的10个解，计算他们的均值和方差，用以生成随机噪声，即作为随机噪声的均值和方差\n",
    "        '''\n",
    "        self.Denoise.train()\n",
    "        # n, d = np.shape(positive_samples)\n",
    "        # indices = np.arange(n)  # indices=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9,..., 30, 31])\n",
    "        \n",
    "        center = np.mean(positive_samples, axis=0)  # (31,1)  axis=0，对第一个维度求均值    下面的 cov 矩阵提供了一个关于这10个样本在31个特征上相互关系的全面视图。\n",
    "        cov = np.cov(positive_samples[:10, :].reshape((self.dim, positive_samples[:10, :].size // self.dim)))#  (10, 31)->(31, 10)  conv=(31,31)  np.cov 函数用于计算协方差矩阵   samples_pool.shape=(10, 31),   \n",
    "\n",
    "        # negative_samples = torch.from_numpy(negative_samples).float()\n",
    "\n",
    "        # 创建一个空的 numpy 数组, 数据增强操作\n",
    "        combined_perturb_x = np.array([])\n",
    "        alphas = [0.1 * i for i in range(1, 21, 2)]   # 10个\n",
    "        for i in alphas:\n",
    "            perturb_x = perturb_solution_along_gradient(positive_samples, r, s, c, i)\n",
    "            \n",
    "            # 将 perturb_x 堆叠到 combined_perturb_x 中\n",
    "            if combined_perturb_x.size == 0:\n",
    "                combined_perturb_x = perturb_x\n",
    "            else:\n",
    "                combined_perturb_x = np.vstack((combined_perturb_x, perturb_x))\n",
    "\n",
    "        # 上采样负样本至100个\n",
    "        upsample_negative_samples = resample(negative_samples, \n",
    "                                        replace=True, \n",
    "                                        n_samples=100,\n",
    "                                        random_state=123)\n",
    "        \n",
    "        n, d = np.shape(combined_perturb_x)\n",
    "        indices = np.arange(n)  # indices=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9,..., 30, 31])\n",
    "        iter_no = (n + self.batchsize - 1) // self.batchsize\n",
    "\n",
    "        for epoch in range(self.epoches):\n",
    "            losses = 0\n",
    "            for iteration in range(iter_no):\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                given_p = combined_perturb_x[iteration * self.batchsize: (1 + iteration) * self.batchsize, :]\n",
    "                given_n = upsample_negative_samples[iteration * self.batchsize: (1 + iteration) * self.batchsize, :]\n",
    "                # loss = self.diffusion_loss_fn(positive_samples, negative_samples, center, cov)\n",
    "                # loss = self.diffusion_loss_fn(X_resampled, downsampled_negative_samples, center, cov)\n",
    "                loss = self.diffusion_loss_fn(given_p, given_n, center, cov)\n",
    "\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.Denoise.parameters(), 1.)\n",
    "                self.optimizer.step()\n",
    "                losses += loss\n",
    "                # print(\"Epoch[{}], loss: {:.5f}\".format(epoch, loss))\n",
    "\n",
    "            # if epoch % 5 == 0:\n",
    "            #     torch.save({\n",
    "            #         'state_dict': self.Denoise.state_dict(),\n",
    "            #     }, 'best_model.pth')\n",
    "\n",
    "\n",
    "            random.shuffle(indices)\n",
    "            combined_perturb_x = combined_perturb_x[indices, :]   # 感觉这里应该加上label = labels[indices, :]\n",
    "            upsample_negative_samples = upsample_negative_samples[indices, :]\n",
    "        # torch.save({\n",
    "        #             'state_dict': self.Denoise.state_dict(),\n",
    "        #         }, 'best_model.pth')\n",
    "\n",
    "    def p_sample_loop(self, x_T, center, cov):\n",
    "        \"\"\"从x[T]恢复x[T-1]、x[T-2]|...x[0]\"\"\"\n",
    "        cur_x = x_T\n",
    "\n",
    "        x_0 = self.p_sample(cur_x, self.num_steps - 1, center, cov)\n",
    "        return x_0\n",
    "\n",
    "    def p_sample(self, x, t, center, cov): # 参数重整化的过程\n",
    "        \"\"\"从x[t]采样t-1时刻的重构值，即从x[t]采样出x[t-1]\"\"\"\n",
    "        t = torch.tensor([t])\n",
    "        x_0 = self.Denoise.diffModel(x,t)\n",
    "\n",
    "        return x_0\n",
    "    \n",
    "\n",
    "    def generate(self, sample_noises, population_size):# population_size=100\n",
    "        self.Denoise.eval()\n",
    "        center = np.mean(sample_noises, axis=0).T\n",
    "        cov = np.cov(sample_noises.T)\n",
    "\n",
    "        noises = np.random.multivariate_normal(center, cov, population_size)\n",
    "        noises = torch.from_numpy(np.maximum(np.minimum(noises, np.ones((population_size, self.dim))),\n",
    "                                             np.zeros((population_size, self.dim)))).float()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            decs= self.p_sample_loop(Variable(noises.cpu()).float(), center, cov).cpu().data.numpy()\n",
    "    \n",
    "        return decs \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# def ndset(A):\n",
    "#     ndsets = []\n",
    "#     for i in range(A.size(0)):\n",
    "#         Mi, Vi = A[i]\n",
    "#         is_dominated = False\n",
    "#         for j in range(A.size(0)):\n",
    "#             Mj, Vj = A[j]\n",
    "#             if (Mi >= Mj and Vi > Vj) or (Mi > Mj and Vi >= Vj):\n",
    "#                 is_dominated = True\n",
    "#                 break\n",
    "#         if not is_dominated:\n",
    "#             ndsets.append(A[i])\n",
    "#     return torch.stack(ndsets)\n",
    "def ndset(A):\n",
    "    dominated = torch.zeros(A.size(0), dtype=torch.bool)\n",
    "    M = A[:, 0].unsqueeze(1)\n",
    "    V = A[:, 1].unsqueeze(1)\n",
    "    \n",
    "    M_dom = (M >= M.T) & (V > V.T) | (M > M.T) & (V >= V.T)\n",
    "    dominated = M_dom.sum(dim=1) > 0\n",
    "\n",
    "    return A[~dominated]\n",
    "\n",
    "def spread(A):\n",
    "    A = ndset(A)\n",
    "    M = -A[:, 0]  # Since pytorch supports slicing, no need to loop through.\n",
    "    V = A[:, 1]\n",
    "    s = torch.sqrt((torch.max(M) - torch.min(M)) ** 2 + (torch.max(V) - torch.min(V)) ** 2)\n",
    "    return s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3713)\n"
     ]
    }
   ],
   "source": [
    "A = torch.randn(8,2)\n",
    "\n",
    "print(spread(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import numpy as np \n",
    "from sklearn.utils import resample\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# 生成一个形状为(100, 31)的随机数组\n",
    "random_array = np.random.rand(100, 31)\n",
    "\n",
    "# 沿着数组的第二个轴（axis=1）计算每行的和\n",
    "row_sums = np.sum(random_array, axis=1)\n",
    "\n",
    "# 使用NumPy的广播（broadcasting）机制进行规范化\n",
    "normalized_array = random_array / row_sums[:, np.newaxis]\n",
    "\n",
    "# 指定positive_samples的索引\n",
    "positive_indices = [0, 2, 5, 9, 10, 11, 30, 22, 32, 12]\n",
    "\n",
    "# 创建包含所有索引的列表\n",
    "all_indices = list(range(100))\n",
    "\n",
    "# 从all_indices中移除positive_indices \n",
    "negative_indices = list(set(all_indices) - set(positive_indices))\n",
    "\n",
    "# 提取positive_samples 和 negative_samples \n",
    "positive_samples = normalized_array[positive_indices, :]\n",
    "negative_samples = normalized_array[negative_indices, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Diffusion(31, 0.001, 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0], loss: -0.58181\n",
      "Epoch[1], loss: -0.59182\n",
      "Epoch[2], loss: -0.60161\n",
      "Epoch[3], loss: -0.60466\n",
      "Epoch[4], loss: -0.60684\n",
      "Epoch[5], loss: -0.62004\n",
      "Epoch[6], loss: -0.62330\n",
      "Epoch[7], loss: -0.63559\n",
      "Epoch[8], loss: -0.63724\n",
      "Epoch[9], loss: -0.64483\n",
      "Epoch[10], loss: -0.65134\n",
      "Epoch[11], loss: -0.65943\n",
      "Epoch[12], loss: -0.66600\n",
      "Epoch[13], loss: -0.67557\n",
      "Epoch[14], loss: -0.68218\n",
      "Epoch[15], loss: -0.69517\n",
      "Epoch[16], loss: -0.69803\n",
      "Epoch[17], loss: -0.69756\n",
      "Epoch[18], loss: -0.71543\n",
      "Epoch[19], loss: -0.71731\n",
      "Epoch[20], loss: -0.72729\n",
      "Epoch[21], loss: -0.73643\n",
      "Epoch[22], loss: -0.74311\n",
      "Epoch[23], loss: -0.75065\n",
      "Epoch[24], loss: -0.75600\n",
      "Epoch[25], loss: -0.76384\n",
      "Epoch[26], loss: -0.77162\n",
      "Epoch[27], loss: -0.77763\n",
      "Epoch[28], loss: -0.78343\n",
      "Epoch[29], loss: -0.79851\n",
      "Epoch[30], loss: -0.80150\n",
      "Epoch[31], loss: -0.80766\n",
      "Epoch[32], loss: -0.81598\n",
      "Epoch[33], loss: -0.82572\n",
      "Epoch[34], loss: -0.83238\n",
      "Epoch[35], loss: -0.83898\n",
      "Epoch[36], loss: -0.84747\n",
      "Epoch[37], loss: -0.85606\n",
      "Epoch[38], loss: -0.86626\n",
      "Epoch[39], loss: -0.87539\n",
      "Epoch[40], loss: -0.87709\n",
      "Epoch[41], loss: -0.88259\n",
      "Epoch[42], loss: -0.89728\n",
      "Epoch[43], loss: -0.90263\n",
      "Epoch[44], loss: -0.91027\n",
      "Epoch[45], loss: -0.92041\n",
      "Epoch[46], loss: -0.92783\n",
      "Epoch[47], loss: -0.93243\n",
      "Epoch[48], loss: -0.94213\n",
      "Epoch[49], loss: -0.94904\n"
     ]
    }
   ],
   "source": [
    "# net.train(positive_samples, negative_samples)\n",
    "net.train(X_resampled, negative_samples)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moea",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
