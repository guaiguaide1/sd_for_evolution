{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self, d, n_noise):  # 1-d vector   d=31, n_noise=31\n",
    "        super(Generator, self).__init__()\n",
    "        self.linear1 = nn.Linear(n_noise, d, bias=True)\n",
    "        self.bn1 = nn.BatchNorm1d(d)\n",
    "        self.linear2 = nn.Linear(d, d, bias=True)\n",
    "        self.bn2 = nn.BatchNorm1d(d)\n",
    "        self.linear3 = nn.Linear(d, d, bias=True)\n",
    "        self.bn3 = nn.BatchNorm1d(d)\n",
    "\n",
    "    \n",
    "    def forward(self, noise):  # noise=(8, 31)\n",
    "        x = torch.tanh(self.bn1(self.linear1(noise)))  # (8,31)->(8,31)\n",
    "        x = torch.tanh(self.bn2(self.linear2(x)))      # (8,31)->(8,31)\n",
    "        x = torch.sigmoid(self.bn3(self.linear3(x)))   # (8,31)->(8,31)\n",
    "        # x = F.softmax(self.bn3(self.linear3(x)), dim=1)   # (8,31)->(8,31)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self, d):  # d=31\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.linear1 = nn.Linear(d, d, bias=True)\n",
    "        self.linear2 = nn.Linear(d, 1, bias=True)\n",
    "\n",
    "    \n",
    "    def forward(self, dec):   # （8， 31）\n",
    "        x = torch.tanh(self.linear1(dec)) # (8, 31)->(8, 31)\n",
    "        x = torch.sigmoid(self.linear2(x)) # (8, 31)->(8, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GAN(object):# d=31, batchsize=8, lr=0.0001, epoches=200, n_noise=31\n",
    "    def __init__(self, d, batchsize, lr, epoches, n_noise):   \n",
    "        self.d = d\n",
    "        self.n_noise = n_noise\n",
    "        self.BCE_loss = nn.BCELoss()\n",
    "        self.G = Generator(self.d, self.n_noise)\n",
    "        self.D = Discriminator(self.d)\n",
    "        # self.G = Generator(self.d, self.n_noise).to(device)\n",
    "        # self.D = Discriminator(self.d).to(device)\n",
    "        self.G.cpu()\n",
    "        self.D.cpu()\n",
    "        self.G_optimizer = optim.Adam(self.G.parameters(), 4*lr)\n",
    "        self.D_optimizer = optim.Adam(self.D.parameters(), lr)\n",
    "        self.epoches = epoches\n",
    "        self.batchsize = batchsize\n",
    "\n",
    "    def train(self, pop_dec, labels, samples_pool):  # pop_dec.shape=(100, 31), labels.shape=(100, 1), samples_pool.shape=(10, 31)\n",
    "        self.D.train()     # samples_pool，是当前种群中表现最好的10个解，计算他们的均值和方差，用以生成随机噪声，即作为随机噪声的均值和方差\n",
    "        self.G.train()\n",
    "        n, d = np.shape(pop_dec)  # n=100,  d=31\n",
    "        indices = np.arange(n)  # indices=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9,..., 98, 99])\n",
    "        \n",
    "        center = np.mean(samples_pool, axis=0)  # (31,1)  axis=0，对第一个维度求均值    下面的 cov 矩阵提供了一个关于这10个样本在31个特征上相互关系的全面视图。\n",
    "        cov = np.cov(samples_pool[:10, :].reshape((d, samples_pool[:10, :].size // d)))#  (10, 31)->(31, 10)  conv=(31,31)  np.cov 函数用于计算协方差矩阵   samples_pool.shape=(10, 31),   \n",
    "        iter_no = (n + self.batchsize - 1) // self.batchsize  # batchsize=8   n=100  iter_no:代表着在给定的设置中，需要多少批（batch）迭代来处理所有 n 个样本。其中，每个批的大小由 self.batchsize（在这个例子中是8）确定。\n",
    "\n",
    "        for epoch in range(self.epoches): # epoches=200\n",
    "            g_train_losses = 0\n",
    "\n",
    "            for iteration in range(iter_no):  # iter_no=13  一共有13个batch, 每个batch有8个样本\n",
    "\n",
    "                \n",
    "                self.D.zero_grad()\n",
    "                given_x = pop_dec[iteration * self.batchsize: (1 + iteration) * self.batchsize, :]   # 一个batchsize的解   given_x=(8, 31)\n",
    "                given_y = labels[iteration * self.batchsize: (1 + iteration) * self.batchsize]   # 对应的一个batchsize的label  given_y=(8,1)\n",
    "                batch_size = np.shape(given_x)[0]  # 因为最后一个batch可能没有8个，所以这里要记录一下batch_size的大小\n",
    "\n",
    "                given_x_ = Variable(torch.from_numpy(given_x).cpu()).float()\n",
    "                given_y = Variable(torch.from_numpy(given_y).cpu()).float()\n",
    "                # given_x_ = Variable(torch.from_numpy(given_x).to(device)).float()\n",
    "                # given_y = Variable(torch.from_numpy(given_y).to(device)).float()\n",
    "                # 在Pytorch0.4.0及以后，Tensor和Variable已经合并\n",
    "                # given_x_ = torch.from_numpy(given_x).to(device).float()   # numpy->tensor   (8, 31)\n",
    "                # given_y = torch.from_numpy(given_y).to(device).float()    # （8，1）\n",
    "                # 注意上面的given_x_, given_y都是真实的数据\n",
    "                # d_results_real = self.D(given_x_.detach())   # 这里应该是不需要detach操作，因为given_x_不是可学习的参数\n",
    "                d_results_real = self.D(given_x_)   # xwf   \n",
    "\n",
    "                # 这里的fake_x就是噪声，将fake_x经过G来生成假的数据, fake_y都是random出来的数据\n",
    "                fake_x = np.random.multivariate_normal(center, cov, batch_size)  # （8， 31）从噪声出发\n",
    "                fake_x = torch.from_numpy(np.maximum(np.minimum(fake_x, np.ones((batch_size, self.d))),\n",
    "                                                         np.zeros((batch_size, self.d))))\n",
    "\n",
    "                fake_y = Variable(torch.zeros((batch_size, 1)).cpu())\n",
    "                fake_x_ = Variable(fake_x.cpu()).float()\n",
    "                # fake_y = torch.zeros((batch_size, 1)).to(device)   # 因为是假的数据嘛，所以fake_y都是0\n",
    "                # fake_x_ = fake_x.to(device).float()\n",
    "\n",
    "                # g_results = self.G(fake_x_.detach())  # g_results=(8,31)   这里写错了，感觉应该是g_results=self.G(fake_x_)    d_results_fake=self.D(g_results.detach)\n",
    "                # d_results_fake = self.D(g_results)  # 因为这里通过g_results会涉及到G的更新，如果这里也设置g_results，则无法梯度回传去更新G\n",
    "                g_results = self.G(fake_x_)          # xwf\n",
    "                d_results_fake = self.D(g_results.detach())\n",
    "\n",
    "                d_train_loss = self.BCE_loss(d_results_real, given_y) + \\\n",
    "                               self.BCE_loss(d_results_fake, fake_y)  \n",
    "                d_train_loss.backward()\n",
    "                self.D_optimizer.step()\n",
    "\n",
    "                \n",
    "                self.G.zero_grad()\n",
    "                fake_x = np.random.multivariate_normal(center, cov, batch_size)\n",
    "                fake_x = torch.from_numpy(np.maximum(np.minimum(fake_x, np.ones((batch_size, self.d))),\n",
    "                                                     np.zeros((batch_size, self.d))))\n",
    "                fake_x_ = Variable(fake_x.cpu()).float()\n",
    "                fake_y = Variable(torch.ones((batch_size, 1)).cpu())\n",
    "                # fake_x_ = fake_x.to(device).float()\n",
    "                # fake_y = torch.ones((batch_size, 1)).to(device)  # 这里你希望G生成的内容经过判别器后能够尽可能地接近1，说明生成的就越真实\n",
    "                g_results = self.G(fake_x_)\n",
    "                d_results = self.D(g_results)\n",
    "                g_train_loss = self.BCE_loss(d_results, fake_y)   \n",
    "                g_train_loss.backward()\n",
    "                self.G_optimizer.step()\n",
    "                g_train_losses += g_train_loss.cpu()\n",
    "                # g_train_losses += g_train_loss.item()\n",
    "\n",
    "            print(\"Epoch[{}], loss: {:.5f}\".format(epoch, g_train_losses))\n",
    "\n",
    "            random.shuffle(indices)\n",
    "            pop_dec = pop_dec[indices, :]   # 感觉这里应该加上label = labels[indices, :]\n",
    "            label = labels[indices, :]   #  xwf\n",
    "\n",
    "    def generate(self, sample_noises, population_size):  # sample_noises.shape=(10, 31)  population_size=100\n",
    "\n",
    "        self.G.eval()  \n",
    "\n",
    "        center = np.mean(sample_noises, axis=0).T   # shape=(31,)\n",
    "        cov = np.cov(sample_noises.T)   # (31, 31)\n",
    "        batch_size = population_size    # bs = 100\n",
    "\n",
    "        noises = np.random.multivariate_normal(center, cov, batch_size)   # (100, 31)\n",
    "        noises = torch.from_numpy(np.maximum(np.minimum(noises, np.ones((batch_size, self.d))),\n",
    "                                                      np.zeros((batch_size, self.d))))\n",
    "        # noises = noises.to(device).float() # 数据移到GPU    (batchsize,n_sample)=(100, 31)   一个batch里面有31个样本，就要预测31个结果，这里有100个batchsize\n",
    "        # with torch.no_grad(): #关闭autograd\n",
    "        #     decs = self.G(noises).cpu().data.numpy() # 生成结果并转回CPU     shape=(100, 31)\n",
    "        decs = self.G(Variable(noises.cpu()).float()).cpu().data.numpy()\n",
    "        return decs\n",
    "\n",
    "    def discrimate(self, off):\n",
    "\n",
    "        self.D.eval()  \n",
    "        batch_size = off.shape[0]\n",
    "        off = off.reshape(batch_size, 1, off.shape[1])\n",
    "        \n",
    "        x = Variable(torch.from_numpy(off).cpu(), volatile=True).float()\n",
    "        d_results = self.D(x).cpu().data.numpy()\n",
    "        # with torch.no_grad():\n",
    "        #     x = torch.from_numpy(off).to(device).float()\n",
    "        #     d_results = self.D(x).cpu().data.numpy()\n",
    "\n",
    "        return d_results.reshape(batch_size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import numpy as np \n",
    "from sklearn.utils import resample\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# 生成一个形状为(100, 31)的随机数组\n",
    "random_array = np.random.rand(100, 31)\n",
    "\n",
    "# 沿着数组的第二个轴（axis=1）计算每行的和\n",
    "row_sums = np.sum(random_array, axis=1)\n",
    "\n",
    "# 使用NumPy的广播（broadcasting）机制进行规范化\n",
    "normalized_array = random_array / row_sums[:, np.newaxis]\n",
    "\n",
    "# 指定positive_samples的索引\n",
    "positive_indices = [0, 2, 5, 9, 10, 11, 30, 22, 32, 12]\n",
    "\n",
    "# 创建包含所有索引的列表\n",
    "all_indices = list(range(100))\n",
    "\n",
    "# 从all_indices中移除positive_indices \n",
    "negative_indices = list(set(all_indices) - set(positive_indices))\n",
    "\n",
    "# 提取positive_samples 和 negative_samples \n",
    "positive_samples = normalized_array[positive_indices, :]\n",
    "negative_samples = normalized_array[negative_indices, :]\n",
    "label = np.zeros((100, 1))\n",
    "label[positive_indices, :] = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = GAN(31, 8, 0.0001, 20, 31)#epoch=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0], loss: 9.73695\n",
      "Epoch[1], loss: 10.11586\n",
      "Epoch[2], loss: 10.51387\n",
      "Epoch[3], loss: 10.93344\n",
      "Epoch[4], loss: 11.36037\n",
      "Epoch[5], loss: 11.78094\n",
      "Epoch[6], loss: 12.20822\n",
      "Epoch[7], loss: 12.65934\n",
      "Epoch[8], loss: 13.11994\n",
      "Epoch[9], loss: 13.57548\n",
      "Epoch[10], loss: 14.03508\n",
      "Epoch[11], loss: 14.52316\n",
      "Epoch[12], loss: 15.00477\n",
      "Epoch[13], loss: 15.49404\n",
      "Epoch[14], loss: 15.96527\n",
      "Epoch[15], loss: 16.45486\n",
      "Epoch[16], loss: 16.95815\n",
      "Epoch[17], loss: 17.43817\n",
      "Epoch[18], loss: 17.92923\n",
      "Epoch[19], loss: 18.42580\n"
     ]
    }
   ],
   "source": [
    "net.train(normalized_array, label, positive_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 31)\n"
     ]
    }
   ],
   "source": [
    "new_sample = net.generate(positive_samples, 100)\n",
    "print(new_sample.shape) # (100,31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are all values within [0, 1]? True\n",
      "Is the sum of each row equal to 1? False\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 生成一个形状为 (100, 31) 的随机数组\n",
    "# random_array = np.random.rand(100, 31)\n",
    "\n",
    "# 检查每一维是否在 0 到 1 之间\n",
    "is_within_range = np.all((new_sample >= 0) & (new_sample <= 1))\n",
    "\n",
    "# 计算每个样本的 31 维之和\n",
    "row_sums = np.sum(new_sample, axis=1)\n",
    "\n",
    "# 检查每个样本的和是否等于 1\n",
    "is_sum_equal_to_1 = np.all(np.isclose(row_sums, 1.0))\n",
    "\n",
    "print(f\"Are all values within [0, 1]? {is_within_range}\")\n",
    "print(f\"Is the sum of each row equal to 1? {is_sum_equal_to_1}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moea",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
